{
 "metadata": {
  "name": "",
  "signature": "sha256:665e5d53b2429b3fd38883dd0e05b00d02938b7253d625642ba1a7ceaf987d1b"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "week 3\n",
      "\n",
      "scalable\n",
      "    able to use thousands of cheap computers to apply to the same problem\n",
      "    scale up vs scale out\n",
      "        scale up - add more main memory\n",
      "        scale out - add more external computers\n",
      "   "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "DNA search problem\n",
      "    ex. \n",
      "        sort the sequences then check at half then see if its greater than string search then search opposite until equal\n",
      "Create Index\n",
      "    create index seq_idx On sequence(seq);\n",
      "        this statement sorts the table sequence by the column seq\n",
      "    this is great because the database will automatically use this index regardless if you call the index\n",
      "Read Trimming\n",
      "    Q. - given set of DNA sequences, trim the final n bps of each sequence and then make new dataset\n",
      "    seperate the amount of sequences into groups and use a worker for each"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Parallel Processing\n",
      "\n",
      "    take objects, split them up, processes them using multiple machines, put all the results back together"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Map Reduce\n",
      "\n",
      "Map - takes in object of some kind and returns another object\n",
      "    corresponds to the size of the origin dataset\n",
      "    think on how to split a dataset into pieces\n",
      "\n",
      "Reduce - takes set of objects and returns an object\n",
      "    corresponds to the amount of groups map creates. Output of Map decides this\n",
      "    \n",
      "hadoop\n",
      "    implementation of map-reduce\n",
      "key idea\n",
      "    map-reduce=high-level programming model and implementation for large-scale parallel data processing"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Data model\n",
      "\n",
      "    files\n",
      "        a file= set with duplicates of (key,value) pairs\n",
      "    map reduce program\n",
      "        input: bag of key value pairs\n",
      "        output: bag of outputkey, value pairs\n",
      " Steps\n",
      "     mapphase - provide map function\n",
      "         input-input key,value\n",
      "         output - intermediate key, value\n",
      "         system applies map function in parallel to all key value pairs in input file\n",
      "     Reduce - reduce function\n",
      "         \n",
      "         takes intermediate key, bag of values\n",
      "         output bag of output(values\n",
      "         system wil group all pairs withthe same intermediate key and passes the bag of values ot teh reduce function"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "#look through document contents then assign a key value pair of the word and a value of 1\n",
      "map(string input_key, string input_value):\n",
      "    // input_key: document name\n",
      "    // input_value: document contents\n",
      "    for each word w in input value:\n",
      "        emitIntermediate(w,1);\n",
      "#shuffle occurs and groups all the key's together into a single group (intermediate_key)\n",
      "\n",
      "\n",
      "# take the output of map and take the word and then sum the values\n",
      "reduce(string intermediate_key, Iterator intermediate_values):\n",
      "    //intermediate_key: word\n",
      "    //intermediate_values: ????\n",
      "    int result = 0;\n",
      "    for each v in intermediate_values:\n",
      "        result +=v;\n",
      "    EmitFinal(intermediate_key, result);"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "Inverted index\n",
      "    word lookup- for each word you can find the tweet that contains that word\n",
      "    \n",
      "    "
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "Parallel Databases\n",
      "\n",
      "NoSQL\n",
      "    low latency, parallel databases\n",
      "HPC - High performance computing\n",
      "    throughput, shared memory\n",
      "Analytics\n",
      "    throughput, parallel databases\n",
      "\n",
      "mapreduce\n",
      "    lightweight framework\n",
      "    automatic parallelization and distrib\n",
      "    fault  tolerance\n",
      "    i/o scheduling\n",
      "    status and monitoring\n",
      "parallel query processing\n",
      "    distributed query\n",
      "        rewrite the query and a union of subqueries\n",
      "        like map phase\n",
      "        ex - SQL\n",
      "    paralell query\n",
      "        each operator is implemented with a parallel algorithm\n",
      "        everything is partitioned across the cluster\n",
      "        ex. mapreduce\n",
      "\n",
      "mapreduce vs parallel databases\n",
      "    parallel databases save time with indexing and views and prepopulated structure\n",
      "    mapreduce allows for fault tolerance and quick initial load times"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": []
    }
   ],
   "metadata": {}
  }
 ]
}